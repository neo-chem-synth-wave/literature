# (20220131, Irwin, R. et al.) Chemformer: A Pre-trained Transformer for Computational Chemistry

Tags: single-step-retrosynthesis, single-step-synthesis, template-free

## Publication Overview

| **Title:**  | Chemformer: A Pre-trained Transformer for Computational Chemistry |
| --- | --- |
| **Authors:**  | Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum |
| Publication Date**:**  | 2022/01/31 |
| Publication Links: | [**IOP Science Machine Learning: Science and Technology**](https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffb) |
| Alternative Links: | [**ChemRxiv](https://chemrxiv.org/engage/chemrxiv/article-details/60ee8a3eb95bdd06d062074b) | [ResearchGate](https://www.researchgate.net/publication/353278829_Chemformer_A_Pre-Trained_Transformer_for_Computational_Chemistry)** |
| Code Links: | [**Official GitHub Repository**](https://github.com/MolecularAI/Chemformer) |

## Publication Abstract

<aside>
ℹ️ Transformer models coupled with a simplified molecular line entry system (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present the Chemformer model - a Transformer-based model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top-1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication.

</aside>