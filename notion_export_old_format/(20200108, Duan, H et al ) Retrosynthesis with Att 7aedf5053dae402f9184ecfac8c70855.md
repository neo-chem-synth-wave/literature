# (20200108, Duan, H. et al.) Retrosynthesis with Attention-based NMT Model and Chemical Analysis of “Wrong” Predictions

Tags: single-step-retrosynthesis, template-free

## Publication Overview

| **Title:**  | Retrosynthesis with Attention-based NMT Model and Chemical Analysis of “Wrong”
Predictions |
| --- | --- |
| **Authors:**  | Hongliang Duan, Ling Wang, Chengyun Zhang, Lin Guo, Jianjun Li |
| Publication Date**:**  | 2020/01/08 |
| Publication Links: | [**RSC Advances**](https://pubs.rsc.org/en/content/articlelanding/2020/ra/c9ra08535a) |
| Alternative Links: | [**arXiv](https://arxiv.org/abs/1908.00727) | [ResearchGate](https://www.researchgate.net/publication/338465368_Retrosynthesis_with_attention-based_NMT_model_and_chemical_analysis_of_wrong_predictions)** |
| Code Links: | [**Official GitHub Repository**](https://github.com/hongliangduan/RetroSynthesisT2T) |

## Publication Abstract

<aside>
ℹ️ We consider retrosynthesis to be a machine translation problem. Accordingly, we apply an attention-based and completely data-driven model named Tensor2Tensor to a data set comprising approximately 50 000 diverse reactions extracted from the United States patent literature. The model significantly outperforms the seq2seq model (37.4%), with top-1 accuracy reaching 54.1%. We also offer a novel insight into the causes of grammatically invalid SMILES, and conduct a test in which experienced chemists select and analyze the “wrong” predictions that may be chemically plausible but differ from the ground truth. The effectiveness of our model is found to be underestimated and the “true” top-1 accuracy reaches as high as 64.6%.

</aside>