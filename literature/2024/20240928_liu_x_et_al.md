# Overview
**Title:**
RetroCaptioner: Beyond Attention in End-to-End Retrosynthesis Transformer via Contrastively Captioned Learnable Graph Representation

**Authors:**
Liu, X., Ai, C., Yang, H., Dong, R., Tang, J., Zheng, S., and Guo, F. |
Liu, X. et al.

**Publication Date:**
2024/09/28

**Link:**
[Oxford Academic Bioinformatics](https://academic.oup.com/bioinformatics/article/40/9/btae561/7789833)

**Alternative Links:**
[GitHub](https://github.com/guofei-tju/RetroCaptioner)

**Tags:**
single-step-retrosynthesis, template-free, retro-captioner


# Abstract
Motivation

Retrosynthesis identifies available precursor molecules for various and novel compounds.
With the advancements and practicality of language models, Transformer-based models have increasingly been used to automate this process.
However, many existing methods struggle to efficiently capture reaction transformation information, limiting the accuracy and applicability of their predictions.

Results

We introduce RetroCaptioner, an advanced end-to-end, Transformer-based framework featuring a Contrastive Reaction Center Captioner.
This captioner guides the training of dual-view attention models using a contrastive learning approach.
It leverages learned molecular graph representations to capture chemically plausible constraints within a single-step learning process.
We integrate the single-encoder, dual-encoder, and encoderâ€“decoder paradigms to effectively fuse information from the sequence and graph representations of molecules.
This involves modifying the Transformer encoder into a uni-view sequence encoder and a dual-view module.
Furthermore, we enhance the captioning of atomic correspondence between SMILES and graphs.
Our proposed method, RetroCaptioner, achieved outstanding performance with 67.2% in top-1 and 93.4% in top-10 exact matched accuracy on the USPTO-50k dataset, alongside an exceptional SMILES validity score of 99.4%.
In addition, RetroCaptioner has demonstrated its reliability in generating synthetic routes for the drug protokylol.


# Citation
```
@article {20240928_liu_x_et_al,
  author       = { Xiaoyi Liu and Chengwei Ai and Hongpeng Yang and Ruihan Dong and Jijun Tang and Shuangjia Zheng and Fei Guo },
  title        = { RetroCaptioner: beyond attention in end-to-end retrosynthesis transformer via contrastively captioned learnable graph representation },
  journal      = { Bioinformatics },
  year         = { 2024 },
  pages        = { btae561 },
  month        = { 09 },
  volume       = { 40 },
  number       = { 9 },
  issn         = { 1367-4811 },
  doi          = { 10.1093/bioinformatics/btae561 },
  url          = { https://doi.org/10.1093/bioinformatics/btae561 },
  eprint       = { https://academic.oup.com/bioinformatics/article-pdf/40/9/btae561/60195134/btae561.pdf }
}
```
